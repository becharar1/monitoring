[global_tags]

# the agent is important to define the collection strategy
[agent]
  interval = "60s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "0s"
  precision = ""
  omit_hostname = false

[[outputs.opentelemetry]]
  ## Override the default (localhost:4317) OpenTelemetry gRPC service
  ## address:port
   service_address = "otelcol:4317"

[[inputs.docker]]
  endpoint = "tcp://host.docker.internal:2375"

[[inputs.rabbitmq]]
   url = "http://rabbitmq:15672"
   username = "guest"
   password = "guest"

[[inputs.prometheus]]
  ## Oracle exporter integration with telegraf
  urls = ["http://oracle_exporter:9161/metrics"]

## The two processors are important to go seamlessly from prometheus naming to TICK stack
[[processors.pivot]]
  ## Tag to use for naming the new field.
  tag_key = "metric_name"
  ## Field to use as the value of the new field.
  value_key = "gauge"

[[processors.pivot]]
  ## Tag to use for naming the new field.
  tag_key = "status"
  ## Field to use as the value of the new field.
  value_key = "value"

[[inputs.redis]]
  servers = ["tcp://redis:6379"]   
  password="eYVX7EwVmmxKPCDmwMtyKVge8oLd2t81" 


[[inputs.elasticsearch]]
  ## specify a list of one or more Elasticsearch servers
  ## you can add username and password to your url to use basic authentication:
  ## servers = ["http://user:pass@localhost:9200"]
  servers = ["http://elasticsearch:9200"]

  ## Timeout for HTTP requests to the elastic search server(s)
  http_timeout = "5s"

  ## When local is true (the default), the node will read only its own stats.
  ## Set local to false when you want to read the node stats from all nodes
  ## of the cluster.
  local = false

  ## Set cluster_health to true when you want to obtain cluster health stats
  #local = true
  cluster_health = true
  cluster_stats = true
  #cluster_stats_only_from_master = true
   indices_include = ["_all"]

[[inputs.mysql]]
  servers = ["root:rootpassword@tcp(mysql_container)/?tls=false&allowCleartextPasswords=true"]
  gather_slave_status = true
  gather_binary_logs = true
  gather_event_waits = true
   gather_table_lock_waits = true
  gather_table_io_waits =true
  gather_process_list=true
  gather_innodb_metrics=true

[[inputs.sql]]
  driver="mysql"
  dsn = "root:rootpassword@tcp(mysql_container)/?tls=false&allowCleartextPasswords=true"
  name_override="mysql_info"

 [[inputs.sql.query]]
       query="SELECT (SELECT VARIABLE_VALUE  from performance_schema.global_variables where VARIABLE_NAME='max_connections') as max_connections, (SELECT VARIABLE_VALUE from performance_schema.global_variables where VARIABLE_NAME ='query_cache_size') as query_cache_size, (SELECT VARIABLE_VALUE from performance_schema.global_variables where VARIABLE_NAME='long_query_time') as long_query_time"

   [[inputs.sql.query]]
       query="SELECT TABLE_SCHEMA, TABLE_NAME, TABLE_ROWS, AVG_ROW_LENGTH, DATA_LENGTH, INDEX_LENGTH, MAX_DATA_LENGTH, DATA_FREE FROM information_schema.TABLES WHERE TABLE_SCHEMA NOT IN ('information_schema', 'mysql', 'myperf', 'performance_schema', 'sys') AND TABLE_TYPE = 'BASE TABLE'"

[[inputs.sql]]
   driver="mysql"
   dsn = "root:rootpassword@tcp(mysql_container)/?tls=false&allowCleartextPasswords=true"
   name_override="mysql_innodb"
     [[inputs.sql.query]]
        query="SELECT EVENT_NAME, SUM_TIMER_WAIT/1000000000 as wait_ms, count_star from performance_schema.events_waits_summary_global_by_event_name where SUM_TIMER_WAIT>0 and  EVENT_NAME LIKE 'wait/synch/mutex/innodb/%'"
     [[inputs.sql.query]]
          query="SELECT * FROM  INFORMATION_SCHEMA.INNODB_BUFFER_POOL_STATS"
     [[inputs.sql.query]]
          query="SELECT (SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE WHERE PAGE_TYPE LIKE 'IBUF%') AS change_buffer_pages,(SELECT COUNT(*) FROM INFORMATION_SCHEMA.INNODB_BUFFER_PAGE) AS total_pages, (SELECT ((change_buffer_pages/total_pages)*100)) AS change_buffer_page_percentage;"
      [[inputs.sql.query]]
          query="Select * from INFORMATION_SCHEMA.INNODB_TRX"
###############################################################################
#                            PROCESSOR PLUGINS                                #
##############################################################################

[[processors.converter]]
  [processors.converter.fields]
  integer=["max_connections","wal_bytes","shared_buffers","uptime",'replication_delay']
  tag=["relname","schemaname","query","wait_event","state"]

[[processors.rename]]
  [[processors.rename.replace]]
    tag = "dbname"
    dest = "db"

##############################################################################
##                            SERVICE INPUT PLUGINS                            #
################################################################################
## common to all databases
[[inputs.postgresql]]
  address = "host=postgres_container user=postgres password=postgres dbname='postgres'"
    tagexclude=["server"]
    fielddrop=["datname"]
  ignored_databases=["postgres_global"]

[[inputs.postgresql_extensible]]

   address = "host=postgres_container user=postgres password=postgres dbname='postgres'"
   tagexclude=["server","db"]

  [[inputs.postgresql_extensible.query]]
     sqlquery="SELECT count (pg_database.datname) as total FROM pg_database JOIN pg_locks ON pg_database.oid=pg_locks.database GROUP BY datname;"

  [[inputs.postgresql_extensible.query]]
    sqlquery="SELECT pg_database_size(datname)/(1024*1024) as database_size from pg_stat_database;"

  [[inputs.postgresql_extensible.query]]
    sqlquery="SELECT EXTRACT(EPOCH FROM now() - pg_postmaster_start_time()) as uptime;"

  [[inputs.postgresql_extensible.query]]
    sqlquery="SELECT * FROM pg_stat_database_conflicts;"

  [[inputs.postgresql_extensible.query]]
    sqlquery="SELECT CASE WHEN pg_is_in_recovery() and pg_last_wal_receive_lsn()!= pg_last_wal_replay_lsn() THEN EXTRACT(EPOCH FROM now() - pg_last_xact_replay_timestamp()) ELSE 0 END as replication_delay;"

   [[inputs.postgresql_extensible.query]]
     sqlquery="SELECT * from pg_stat_bigwriter;"

   [[inputs.postgresql_extensible.query]]
    sqlquery="SELECT wal_bytes from pg_stat_wal;"

   [[inputs.postgresql_extensible.query]]
     sqlquery="select setting as max_connections from pg_settings where name='max_connections'"

    [[inputs.postgresql_extensible.query]]
      sqlquery="SELECT setting as data_directory FROM  pg_settings WHERE name = 'data_directory'"
